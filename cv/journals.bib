% journals
@article{
dey2025simple,
title={Simple Calibration via Geodesic Kernels},
author={Jayanta Dey and Haoyin Xu and Ashwin De Silva and Joshua T Vogelstein},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2025},
url={https://openreview.net/forum?id=dpcRp8ix5T},
note={}
}

@Article{math12050746,
AUTHOR = {Helm, Hayden and \textbf{De Silva}, \textbf{Ashwin} and Vogelstein, Joshua T. and Priebe, Carey E. and Yang, Weiwei},
TITLE = {Approximately Optimal Domain Adaptation with Fisher’s Linear Discriminant},
JOURNAL = {Mathematics},
VOLUME = {12},
YEAR = {2024},
NUMBER = {5},
ARTICLE-NUMBER = {746},
URL = {https://www.mdpi.com/2227-7390/12/5/746},
ISSN = {2227-7390},
ABSTRACT = {We propose and study a data-driven method that can interpolate between a classical and a modern approach to classification for a class of linear models. The class is the convex combinations of an average of the source task classifiers and a classifier trained on the limited data available for the target task. We derive the expected loss of an element in the class with respect to the target distribution for a specific generative model, propose a computable approximation of the loss, and demonstrate that the element of the proposed class that minimizes the approximated risk is able to exploit a natural bias–variance trade-off in task space in both simulated and real-data settings. We conclude by discussing further applications, limitations, and potential future research directions.},
DOI = {10.3390/math12050746}
}

